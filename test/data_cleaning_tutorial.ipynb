{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixmydata tutorial: cleaning built-in datasets\n\n",
    "This notebook demonstrates how to explore and clean the bundled sample datasets using the `Fixmydata` utilities. Each section mirrors a typical data quality workflow so you can adapt the snippets to your own projects."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "- Install dependencies from `requirements.txt`.\n",
    "- Ensure the project root is on your Python path so `Fixmydata` can be imported directly."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport pandas as pd\nfrom pathlib import Path\n\n# Ensure project root is on the Python path\nROOT = Path().resolve().parent\nif str(ROOT) not in sys.path:\n    sys.path.append(str(ROOT))\n\nDATA_DIR = ROOT / 'datasets'\n\nfrom Fixmydata import DataCleaner, DataValidator, OutlierDetector"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Titanic-style passenger data\n\n",
    "We will use `datasets/tested.csv`, which mirrors the familiar Titanic competition data."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(418, 12)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   PassengerId  Survived  Pclass                                          Name     Sex  ...  Parch   Ticket     Fare Cabin  Embarked\n0          892         0       3                              Kelly, Mr. James    male  ...      0   330911   7.8292   NaN         Q\n1          893         1       3              Wilkes, Mrs. James (Ellen Needs)  female  ...      0   363272   7.0000   NaN         S\n2          894         0       2                     Myles, Mr. Thomas Francis    male  ...      0   240276   9.6875   NaN         Q\n3          895         0       3                              Wirz, Mr. Albert    male  ...      0   315154   8.6625   NaN         S\n4          896         1       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female  ...      1  3101298  12.2875   NaN         S\n\n[5 rows x 12 columns]"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "titanic_path = DATA_DIR / 'tested.csv'\n",
    "titanic_df = pd.read_csv(titanic_path)\n",
    "\n",
    "print(titanic_df.shape)\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect missing values\n",
    "Before cleaning, it is useful to see which columns contain gaps."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "             missing_values\nPassengerId               0\nSurvived                  0\nPclass                    0\nName                      0\nSex                       0\nAge                      86\nSibSp                     0\nParch                     0\nTicket                    0\nFare                      1\nCabin                   327\nEmbarked                  0"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "titanic_df.isnull().sum().to_frame('missing_values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean the passenger data\n",
    "We will standardize column names, fill missing numeric values with the median, fill categorical gaps with the mode, and remove duplicates. The `DataCleaner` instance keeps track of the working DataFrame internally."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   passengerid  survived  pclass                                          name  ...   ticket     fare            cabin  embarked\n0          892         0       3                              Kelly, Mr. James  ...   330911   7.8292  B57 B59 B63 B66         Q\n1          893         1       3              Wilkes, Mrs. James (Ellen Needs)  ...   363272   7.0000  B57 B59 B63 B66         S\n2          894         0       2                     Myles, Mr. Thomas Francis  ...   240276   9.6875  B57 B59 B63 B66         Q\n3          895         0       3                              Wirz, Mr. Albert  ...   315154   8.6625  B57 B59 B63 B66         S\n4          896         1       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  ...  3101298  12.2875  B57 B59 B63 B66         S\n\n[5 rows x 12 columns]"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": "cleaner = DataCleaner(titanic_df)\n\n# Normalize headers for easier downstream processing\ncleaner.standardize_columns()\n\n# Replace missing numeric values with medians\ncleaner.fill_missing(strategy='median', columns=['age', 'fare'])\n\n# Fill categorical gaps using the most frequent values\ncleaner.fill_missing(strategy='mode')\n\n# Drop accidental duplicate rows if any\ntitanic_clean = cleaner.remove_duplicates()\n\ntitanic_clean.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the cleaned data\n",
    "`DataValidator` can assert common expectations. Here we ensure the DataFrame is non-empty and that passenger ages fall inside a reasonable range."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              age        fare\ncount  418.000000  418.000000\nmean    29.599282   35.576535\nstd     12.703770   55.850103\nmin      0.170000    0.000000\n25%     23.000000    7.895800\n50%     27.000000   14.454200\n75%     35.750000   31.471875\nmax     76.000000  512.329200"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "validator = DataValidator(titanic_clean)\n",
    "validator.validate_non_empty()\n",
    "validator.validate_range('age', 0, 90)\n",
    "\n",
    "titanic_clean[['age', 'fare']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect and remove outliers\n",
    "We can use `OutlierDetector` to filter extreme values. The IQR method is robust for skewed distributions like fares."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original rows: 418\nRows after IQR filtering: 281\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              age        fare\ncount  281.000000  281.000000\nmean    28.272242   15.618090\nstd      7.876031   12.818909\nmin     12.000000    0.000000\n25%     24.000000    7.775000\n50%     27.000000    8.662500\n75%     30.000000   21.000000\nmax     54.000000   65.000000"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "detector = OutlierDetector(titanic_clean)\n",
    "titanic_iqr = detector.iqr_outliers()\n",
    "\n",
    "print('Original rows:', len(titanic_clean))\n",
    "print('Rows after IQR filtering:', len(titanic_iqr))\n",
    "\n",
    "titanic_iqr[['age', 'fare']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore the USA housing data\n",
    "The `USA Housing Dataset.csv` contains home sale information. The same cleaners can be applied to prepare the data for modeling."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "                  date      price  bedrooms  bathrooms  ...                   street       city  statezip  country\n0  2014-05-09 00:00:00   376000.0       3.0       2.00  ...  9245-9249 Fremont Ave N    Seattle  WA 98103      USA\n1  2014-05-09 00:00:00   800000.0       4.0       3.25  ...         33001 NE 24th St  Carnation  WA 98014      USA\n2  2014-05-09 00:00:00  2238888.0       5.0       6.50  ...         7070 270th Pl SE   Issaquah  WA 98029      USA\n3  2014-05-09 00:00:00   324000.0       3.0       2.25  ...           820 NW 95th St    Seattle  WA 98117      USA\n4  2014-05-10 00:00:00   549900.0       5.0       2.75  ...        10834 31st Ave SW    Seattle  WA 98146      USA\n\n[5 rows x 18 columns]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "housing_path = DATA_DIR / 'USA Housing Dataset.csv'\n",
    "housing_df = pd.read_csv(housing_path)\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean housing records and compute quick insights\n",
    "We standardize column names, fill any numeric gaps with column means, and check the relationship between living area and sale price after removing Z-score outliers."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Correlation between price and square footage: 0.611\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "              price  sqft_living     bedrooms   bathrooms\ncount  3.805000e+03  3805.000000  3805.000000  3805.00000\nmean   4.991892e+05  2019.579763     3.349803     2.09159\nstd    2.716459e+05   786.018440     0.855091     0.70747\nmin    0.000000e+00   370.000000     1.000000     0.75000\n25%    3.128910e+05  1430.000000     3.000000     1.75000\n50%    4.448450e+05  1910.000000     3.000000     2.25000\n75%    6.200000e+05  2500.000000     4.000000     2.50000\nmax    2.300000e+06  4960.000000     6.000000     4.50000"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "housing_cleaner = DataCleaner(housing_df)\n",
    "housing_cleaner.standardize_columns()\n",
    "housing_cleaner.fill_missing(strategy='mean')\n",
    "housing_base = housing_cleaner.remove_duplicates()\n",
    "\n",
    "housing_detector = OutlierDetector(housing_base)\n",
    "housing_no_outliers = housing_detector.z_score_outliers(threshold=3)\n",
    "\n",
    "price_sqft_corr = housing_no_outliers['price'].corr(housing_no_outliers['sqft_living'])\n",
    "print(f'Correlation between price and square footage: {price_sqft_corr:.3f}')\n",
    "housing_no_outliers[['price', 'sqft_living', 'bedrooms', 'bathrooms']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Swap in your own CSV paths and reuse the same cleaning steps.\n",
    "- Try different fill strategies (mean/median/mode) depending on the data type.\n",
    "- Adjust outlier thresholds to balance robustness and recall.\n",
    "- Add additional validation checks before training models or generating reports."
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}