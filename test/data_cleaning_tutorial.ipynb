{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fixmydata tutorial: cleaning built-in datasets\n\n",
        "This notebook demonstrates how to explore and clean the bundled sample datasets using the `Fixmydata` utilities. Each section mirrors a typical data quality workflow so you can adapt the snippets to your own projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "- Install dependencies from `requirements.txt`.\n",
        "- Ensure the project root is on your Python path so `Fixmydata` can be imported directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from Fixmydata import DataCleaner, DataValidator, OutlierDetector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the Titanic-style passenger data\n\n",
        "We will use `datasets/tested.csv`, which mirrors the familiar Titanic competition data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titanic_path = '../datasets/tested.csv'\n",
        "titanic_df = pd.read_csv(titanic_path)\n",
        "\n",
        "print(titanic_df.shape)\n",
        "titanic_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspect missing values\n",
        "Before cleaning, it is useful to see which columns contain gaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titanic_df.isnull().sum().to_frame('missing_values')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clean the passenger data\n",
        "We will standardize column names, fill missing numeric values with the median, and remove duplicates. The `DataCleaner` instance keeps track of the working DataFrame internally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleaner = DataCleaner(titanic_df)\n",
        "\n",
        "# Normalize headers for easier downstream processing\n",
        "cleaner.standardize_columns()\n",
        "\n",
        "# Replace missing ages and fares with their median values\n",
        "cleaner.fill_missing(strategy='median', columns=['age', 'fare'])\n",
        "\n",
        "# Drop accidental duplicate rows if any\n",
        "titanic_clean = cleaner.remove_duplicates()\n",
        "\n",
        "titanic_clean.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validate the cleaned data\n",
        "`DataValidator` can assert common expectations. Here we ensure the DataFrame is non-empty and that passenger ages fall inside a reasonable range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "validator = DataValidator(titanic_clean)\n",
        "validator.validate_non_empty()\n",
        "validator.validate_range('age', 0, 90)\n",
        "\n",
        "titanic_clean[['age', 'fare']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detect and remove outliers\n",
        "We can use `OutlierDetector` to filter extreme values. The IQR method is robust for skewed distributions like fares."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detector = OutlierDetector(titanic_clean)\n",
        "titanic_iqr = detector.iqr_outliers()\n",
        "\n",
        "print('Original rows:', len(titanic_clean))\n",
        "print('Rows after IQR filtering:', len(titanic_iqr))\n",
        "\n",
        "titanic_iqr[['age', 'fare']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Explore the USA housing data\n",
        "The `USA Housing Dataset.csv` contains home sale information. The same cleaners can be applied to prepare the data for modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_path = '../datasets/USA Housing Dataset.csv'\n",
        "housing_df = pd.read_csv(housing_path)\n",
        "housing_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Clean housing records and compute quick insights\n",
        "We standardize column names, fill any numeric gaps with column means, and check the relationship between living area and sale price after removing Z-score outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "housing_cleaner = DataCleaner(housing_df)\n",
        "housing_cleaner.standardize_columns()\n",
        "housing_cleaner.fill_missing(strategy='mean')\n",
        "housing_base = housing_cleaner.remove_duplicates()\n",
        "\n",
        "housing_detector = OutlierDetector(housing_base)\n",
        "housing_no_outliers = housing_detector.z_score_outliers(threshold=3)\n",
        "\n",
        "price_sqft_corr = housing_no_outliers['price'].corr(housing_no_outliers['sqft_living'])\n",
        "print(f'Correlation between price and square footage: {price_sqft_corr:.3f}')\n",
        "housing_no_outliers[['price', 'sqft_living', 'bedrooms', 'bathrooms']].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "- Swap in your own CSV paths and reuse the same cleaning steps.\n",
        "- Try different fill strategies (mean/median/mode) depending on the data type.\n",
        "- Adjust outlier thresholds to balance robustness and recall.\n",
        "- Add additional validation checks before training models or generating reports."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}