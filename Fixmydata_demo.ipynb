{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Fixmydata Project Walkthrough\n\nThis notebook demonstrates how the **Fixmydata** library cleans, validates, and inspects a dataset end-to-end. It also contains a ready-to-present outline for a 5\u20137 minute project presentation."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Overview\n\nFixmydata wraps common pandas cleaning and validation patterns in small, composable helpers:\n\n- **`DataCleaner`**: remove duplicates, drop/fill missing data, trim whitespace, and drop columns.\n- **`DataValidator`**: assert numeric ranges and check for empty datasets.\n- **`OutlierDetector`**: filter outliers with Z-score or IQR methods while ignoring non-numeric fields.\n\nThe workflow keeps a clean, chainable `DataFrame` in memory so you can move from messy input to validated analytics quickly."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import pandas as pd\nfrom Fixmydata import DataCleaner, DataValidator, OutlierDetector"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Load a messy sample dataset\n\nThe sample data intentionally includes:\n- Duplicate IDs and trailing whitespace in `city`.\n- Missing `city` and `price` values.\n- An obvious price outlier."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "raw = pd.DataFrame({\n    \"id\": [1, 1, 2, 3, 4, 5],\n    \"city\": [\"  New York\", \"Boston  \", \"Chicago\", None, \"San Francisco\", \"Houston\"],\n    \"price\": [10.5, 9.7, 11.2, 13.0, None, 99.0],\n})\nraw"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Clean the data\n\n- Remove duplicate IDs (keep the first occurrence).\n- Drop rows missing a `city` value.\n- Standardize whitespace for the `city` column.\n- Fill missing prices with the column median for analysis."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "cleaner = DataCleaner(raw)\ncleaner.remove_duplicates(subset=[\"id\"])\ncleaner.drop_missing(columns=[\"city\"])\ncleaner.standardize_whitespace([\"city\"])\nmedian_price = cleaner.data[\"price\"].median()\ncleaner.fill_missing(\"price\", median_price)\nclean = cleaner.data\nclean"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Validate the cleaned dataset\n\n- Confirm the dataframe is non-empty and free of nulls.\n- Ensure `price` stays within a practical range (0\u201350)."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "validator = DataValidator(clean)\nvalidator.validate_non_empty()\nvalidator.validate_range(\"price\", 0, 50)\nclean"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Filter outliers\n\nUse Z-score filtering (threshold 2.5) to keep only inlier rows while safely ignoring non-numeric columns."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "outlier_detector = OutlierDetector(clean)\ninliers = outlier_detector.z_score_outliers(threshold=2.5)\ninliers"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Summary of results\n\n- Started with duplicate IDs, whitespace issues, missing values, and a price outlier.\n- Cleaned dataset now has standardized city names and imputed prices.\n- Validation confirms completeness and realistic price ranges.\n- Outlier detection isolates reliable rows for downstream analysis."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Presentation outline (5\u20137 minutes)\n\nUse this as a talk track\u2014each bullet should take ~30\u201360 seconds.\n\n1. **Problem & goal (45s)**: Data quality slows analysis; Fixmydata packages repeatable fixes on top of pandas.\n2. **Library overview (60s)**: Briefly introduce `DataCleaner`, `DataValidator`, `OutlierDetector`, and helper utilities.\n3. **Workflow demo (2\u20133m)**:\n   - Load messy sample data and show issues (duplicates, whitespace, nulls, outlier).\n   - Run cleaning steps and display the cleaned frame.\n   - Validate ranges and completeness; highlight guardrail errors.\n   - Filter outliers with Z-score or IQR; note automatic numeric-column handling.\n4. **Implementation highlights (60\u201390s)**: Mention small, focused classes, explicit error messages, and chainable methods that keep a copy of the data.\n5. **Practical uses (45s)**: Quick pre-EDA cleanup, lightweight pipeline steps, or teaching data quality basics.\n6. **Next steps (30\u201345s)**: Extend validators, add schema configs, or ship formal docs.\n\n**Call to action**: Invite questions or contributions via the repository."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}